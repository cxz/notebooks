{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform train & test.csv into numpy arrays.\n",
    "\n",
    "Before starting train.csv is split into parts so we can read in parallel.\n",
    "\n",
    "- `split -l 10000000 train.csv train_part`\n",
    "- `for i in $(ls -1 train_part*); do sed -i '1s;^;ip,app,device,os,channel,click_time,attributed_time,is_attributed\\n;' $i`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import hashlib\n",
    "import traceback \n",
    "from csv import DictReader\n",
    "import multiprocessing as mp\n",
    "from functools import lru_cache\n",
    "import math\n",
    "from collections import Counter\n",
    "from pyhashxx import hashxx\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "D = 2 ** 23\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "    \n",
    "    # single features\n",
    "    ('ip',), \n",
    "    ('app',), \n",
    "    ('device',),\n",
    "    ('os',), \n",
    "    ('channel',),\n",
    "    \n",
    "    ('click_hour',),\n",
    "    \n",
    "    # pair interactions\n",
    "    ('device', 'app'), \n",
    "    ('channel', 'app'), \n",
    "    ('channel', 'device'), \n",
    "    ('channel', 'os'),\n",
    "    ('ip', 'channel'),\n",
    "    ('ip', 'device'),\n",
    "    ('ip', 'app'),\n",
    "    ('ip', 'click_hour'),\n",
    "    \n",
    "    # triple\n",
    "    ('ip', 'device', 'os')\n",
    "]\n",
    "\n",
    "COUNT_FEATURES = [\n",
    "    ('device', 'app'), \n",
    "    ('channel', 'app'), \n",
    "    ('channel', 'device'), \n",
    "    ('channel', 'os'),\n",
    "    ('ip', 'channel'),\n",
    "    ('ip', 'device'),\n",
    "    ('ip', 'app'),\n",
    "    ('ip', 'click_hour'),\n",
    "    ('ip', 'device', 'os')        \n",
    "]\n",
    "\n",
    "# maxsize=None means the cache is unbounded.\n",
    "# set to something reasonable if memory is limited.\n",
    "@lru_cache(maxsize=None)\n",
    "def hashed(value, D):\n",
    "    # hash is not stable after python 3.3 unless PYTHONHASHSEED is set.\n",
    "    # we need something with less collisions and stable to be able to pickle the model.\n",
    "    #return int(hashlib.md5(value.encode('utf8')).hexdigest(), 16) % D\n",
    "    return hashxx(value.encode('utf8')) % D\n",
    "\n",
    "        \n",
    "def get_x(csv_row):        \n",
    "    try:\n",
    "        x = {}\n",
    "        csv_row['click_hour'] = int(csv_row['click_time'][-8:-6]) # hour\n",
    "        for k in FEATURES:\n",
    "            x[k] = hashed(' '.join([str(csv_row[c]) for c in k]), D)\n",
    "        return x\n",
    "    except Exception as e:\n",
    "        #print(csv_row)        \n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def load_part(fname, max_size=10000000):\n",
    "    train_x = np.zeros((max_size, len(FEATURES)), dtype=np.uint32) \n",
    "    train_y = np.zeros((max_size), dtype=np.uint8)\n",
    "    assert(D < 2**32)\n",
    "    \n",
    "    partial_counters = {}\n",
    "    for k in COUNT_FEATURES:    \n",
    "        partial_counters[k] = Counter()\n",
    "        \n",
    "    with open(fname) as f:\n",
    "        rows = 0\n",
    "        for idx, row in tqdm(enumerate(DictReader(f)), total=max_size, mininterval=30):\n",
    "            h = get_x(row)\n",
    "            x = [h[k] for k in FEATURES]\n",
    "            \n",
    "            train_x[idx, :] = x\n",
    "            train_y[idx] = 1. if row['is_attributed'] == '1' else 0\n",
    "            \n",
    "            # partial counters\n",
    "            for k in COUNT_FEATURES:                \n",
    "                partial_counters[k][h[k]] += 1\n",
    "                \n",
    "            rows += 1\n",
    "            \n",
    "        return train_x[:rows], train_y[:rows], partial_counters\n",
    "\n",
    "def prepare_train():\n",
    "    fnames = list(glob.glob('input/train_parta*'))\n",
    "\n",
    "    p = mp.Pool(8)\n",
    "    parts = p.map(load_part, fnames)\n",
    "    \n",
    "    X = np.concatenate([p[0] for p in parts])\n",
    "    y = np.concatenate([p[1] for p in parts])\n",
    "    counters = {}\n",
    "    for k in COUNT_FEATURES:                \n",
    "        counters[k] = sum([p[2][k] for p in parts], Counter())\n",
    "    \n",
    "    print(X.shape, y.shape)\n",
    "    p.close()\n",
    "    \n",
    "    d = int(math.log(D, 2))\n",
    "    np.savez_compressed('tmp/train-hashxx-D{}.npz'.format(d), x=X, y=y, features=FEATURES)\n",
    "    np.savez_compressed('tmp/aux-hashxx-D{}.npz'.format(d), counters=counters, features=FEATURES)\n",
    "    return X, y, counters\n",
    "\n",
    "def prepare_test():\n",
    "    size = 18790470\n",
    "    X = np.zeros((size, len(FEATURES)), dtype=np.uint32)\n",
    "    click_id = np.zeros((size), dtype=np.uint32)\n",
    "    with open('input/test.csv') as f:\n",
    "        for idx, row in tqdm(enumerate(DictReader(f)), total=size, mininterval=30):\n",
    "            h = get_x(row)\n",
    "            x = [h[k] for k in FEATURES]\n",
    "            X[idx, :] = x\n",
    "            click_id[idx] = row['click_id']\n",
    "    d = int(math.log(D, 2))\n",
    "    np.savez_compressed('tmp/test-hashxx-D{}.npz'.format(d), x=X, click_id=click_id)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 2136732/18790470 [01:40<13:00, 21323.91it/s]"
     ]
    }
   ],
   "source": [
    "#X, y, counters = prepare_train()\n",
    "#print('finished train.')\n",
    "\n",
    "X = prepare_test()\n",
    "print('finished test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((184903890, 15), (184903890,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape\n",
    "#import gc\n",
    "#del X\n",
    "#del y\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    train = np.load('tmp/train.npz')\n",
    "    X_train, y_train = train['x'], train['y']\n",
    "    print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    test = np.load('tmp/test.npz')\n",
    "    X_test = test['x']\n",
    "    print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
