{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv.zip', 'test.csv', 'sample_submission.csv.zip', 'test.csv.zip', 'train.csv', 'sample_submission.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Print all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "IS_LOCAL = True\n",
    "\n",
    "import os\n",
    "\n",
    "if IS_LOCAL:\n",
    "    PATH=\"input\"\n",
    "else:\n",
    "    PATH=\"../input\"\n",
    "    \n",
    "print(os.listdir(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e87c173d93d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    707\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1069\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'as_recarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1837\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_integer_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_integer_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m     \"\"\"\n\u001b[1;32m    779\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mprovided\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0man\u001b[0m \u001b[0minteger\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n",
    "test_df = pd.read_csv(os.path.join(PATH, \"test.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape: (4459, 4993), test shape: (49342, 4992)\n",
      "ntrain:  4459  ntest:  49342\n"
     ]
    }
   ],
   "source": [
    "dtypes = train_df.dtypes.to_dict()\n",
    "print(f\"train shape: {train_df.shape}, test shape: {test_df.shape}\")\n",
    "\n",
    "train_target = np.log1p(train_df['target'].values)\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ntest  = test_df.shape[0]\n",
    "print('ntrain: ', ntrain, ' ntest: ', ntest)\n",
    "\n",
    "ignored_columns = ['ID', 'target']\n",
    "feature_columns = [c for c in train_df.columns if c not in ignored_columns]    \n",
    "\n",
    "# concatencate everything train + test...\n",
    "df_all = pd.concat([train_df[feature_columns], test_df[feature_columns]]).astype(np.float32)\n",
    "\n",
    "dtype_counter = Counter()\n",
    "unique_counter = Counter()\n",
    "for c in feature_columns:\n",
    "    dtype_str = str(dtypes[c])\n",
    "    dtype_counter[dtype_str] += 1\n",
    "    unique = len(set(train_df[c].values))\n",
    "    unique_counter[unique] += 1\n",
    "    if unique == 1:\n",
    "        ignored_columns.append(c)\n",
    "        \n",
    "df_all = df_all[[c for c in df_all.columns if c not in ignored_columns]]\n",
    "train_data = df_all.iloc[0:ntrain, :]\n",
    "test_data  = df_all.iloc[ntrain:, :]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('cache/traintest.pkl', 'wb') as f:\n",
    "    pickle.dump([train_data, test_data, train_target], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cache/traintest.pkl', 'rb') as f:\n",
    "    train_data, test_data, train_y = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD, FactorAnalysis\n",
    "from sklearn.random_projection import GaussianRandomProjection, SparseRandomProjection\n",
    "\n",
    "def build_decomposition2(df_train, df_test, n_comp):\n",
    "    tfa = FactorAnalysis(n_components=n_comp, random_state=0)\n",
    "    tfa_results_train = tfa.fit_transform(df_train)\n",
    "    tfa_results_test = tfa.transform(df_test)    \n",
    "        \n",
    "    srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=0)\n",
    "    srp_results_train = srp.fit_transform(df_train)\n",
    "    srp_results_test = srp.transform(df_test)\n",
    "    \n",
    "    out_train = pd.DataFrame()\n",
    "    out_test = pd.DataFrame()\n",
    "    \n",
    "    # Append decomposition components to datasets\n",
    "    for i in range(n_comp):\n",
    "        out_train['tfa_' + str(i)] = tfa_results_train[:, i]\n",
    "        out_test['tfa_' + str(i)] = tfa_results_test[:, i]\n",
    "\n",
    "        out_train['srp_' + str(i)] = srp_results_train[:, i]\n",
    "        out_test['srp_' + str(i)] = srp_results_test[:, i]\n",
    "\n",
    "    return out_train, out_test\n",
    "\n",
    "    \n",
    "\n",
    "def build_decomposition(df_train, df_test, n_comp):\n",
    "    # tSVD\n",
    "    print('tsvd')\n",
    "    tsvd = TruncatedSVD(n_components=n_comp, random_state=0)\n",
    "    tsvd_results_train = tsvd.fit_transform(df_train)\n",
    "    tsvd_results_test = tsvd.transform(df_test)\n",
    "\n",
    "    # PCA\n",
    "    print('pca')\n",
    "    pca = PCA(n_components=n_comp, random_state=0)\n",
    "    pca2_results_train = pca.fit_transform(df_train)\n",
    "    pca2_results_test = pca.transform(df_test)\n",
    "\n",
    "    # ICA\n",
    "    print('ica')\n",
    "    ica = FastICA(n_components=n_comp, random_state=0)\n",
    "    ica2_results_train = ica.fit_transform(df_train)\n",
    "    ica2_results_test = ica.transform(df_test)\n",
    "\n",
    "    # GRP\n",
    "    print('grp')\n",
    "    grp = GaussianRandomProjection(n_components=n_comp, eps=0.1, random_state=0)\n",
    "    grp_results_train = grp.fit_transform(df_train)\n",
    "    grp_results_test = grp.transform(df_test)\n",
    "\n",
    "    # SRP\n",
    "    print('srp')\n",
    "    srp = SparseRandomProjection(n_components=n_comp, dense_output=True, random_state=0)\n",
    "    srp_results_train = srp.fit_transform(df_train)\n",
    "    srp_results_test = srp.transform(df_test)\n",
    "\n",
    "    out_train = pd.DataFrame()\n",
    "    out_test = pd.DataFrame()\n",
    "    \n",
    "    # Append decomposition components to datasets\n",
    "    for i in range(n_comp):\n",
    "        out_train['pca_' + str(i)] = pca2_results_train[:, i]\n",
    "        out_test['pca_' + str(i)] = pca2_results_test[:, i]\n",
    "\n",
    "        out_train['ica_' + str(i)] = ica2_results_train[:, i]\n",
    "        out_test['ica_' + str(i)] = ica2_results_test[:, i]\n",
    "\n",
    "        out_train['tsvd_' + str(i)] = tsvd_results_train[:, i]\n",
    "        out_test['tsvd_' + str(i)] = tsvd_results_test[:, i]\n",
    "\n",
    "        out_train['grp_' + str(i)] = grp_results_train[:, i]\n",
    "        out_test['grp_' + str(i)] = grp_results_test[:, i]\n",
    "\n",
    "        out_train['srp_' + str(i)] = srp_results_train[:, i]\n",
    "        out_test['srp_' + str(i)] = srp_results_test[:, i]\n",
    "\n",
    "    return out_train, out_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsvd\n",
      "pca\n",
      "ica\n",
      "grp\n",
      "srp\n"
     ]
    }
   ],
   "source": [
    "out50_train, out50_test = build_decomposition(train_data, test_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('cache/traintest_decomposition50.pkl', 'wb') as f:\n",
    "    pickle.dump([out50_train, out50_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsvd\n",
      "pca\n",
      "ica\n",
      "grp\n",
      "srp\n"
     ]
    }
   ],
   "source": [
    "out100_train, out100_test = build_decomposition(train_data, test_data, 100)\n",
    "with open('cache/traintest_decomposition100.pkl', 'wb') as f:\n",
    "    pickle.dump([out100_train, out100_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "out50_train, out50_test = build_decomposition2(train_data, test_data, 50)\n",
    "with open('cache/traintest_decomposition2_50.pkl', 'wb') as f:\n",
    "    pickle.dump([out50_train, out50_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile(x, q):\n",
    "    return np.percentile(x, q=q)\n",
    "\n",
    "def aggregates(df_train, df_test):\n",
    "    import pandas as pd\n",
    "    from scipy.stats import skew, kurtosis\n",
    "    def aggregate_row(row):\n",
    "        non_zero_values = row.iloc[row.nonzero()].dropna(how='all')\n",
    "        aggs = {'non_zero_mean': non_zero_values.mean(),\n",
    "                'non_zero_std': non_zero_values.std(),\n",
    "                'non_zero_max': non_zero_values.max(),\n",
    "                'non_zero_min': non_zero_values.min(),\n",
    "                'non_zero_sum': non_zero_values.sum(),\n",
    "                'non_zero_skewness': skew(non_zero_values),\n",
    "                'non_zero_kurtosis': kurtosis(non_zero_values),\n",
    "                'non_zero_median': non_zero_values.median(),\n",
    "                #'non_zero_q1': quantile(non_zero_values, 25),\n",
    "                #'non_zero_q3': quantile(non_zero_values, 75),\n",
    "                'non_zero_log_mean': np.log1p(non_zero_values).mean(),\n",
    "                'non_zero_log_std': np.log1p(non_zero_values).std(),\n",
    "                'non_zero_log_max': np.log1p(non_zero_values).max(),\n",
    "                'non_zero_log_min': np.log1p(non_zero_values).min(),\n",
    "                'non_zero_log_sum': np.log1p(non_zero_values).sum(),\n",
    "                'non_zero_log_skewness': skew(np.log1p(non_zero_values)),\n",
    "                'non_zero_log_kurtosis': kurtosis(np.log1p(non_zero_values)),\n",
    "                'non_zero_log_median': np.log1p(non_zero_values).median(),\n",
    "                #'non_zero_log_q1': quantile(np.log1p(non_zero_values), 25),\n",
    "                #'non_zero_log_q3': quantile(np.log1p(non_zero_values), 75),\n",
    "                'non_zero_count': non_zero_values.count(),\n",
    "                'non_zero_fraction': non_zero_values.count() / row.count()\n",
    "                }\n",
    "        return pd.Series(aggs)\n",
    "    agg_train = df_train.apply(aggregate_row, axis=1)\n",
    "    agg_test = df_test.apply(aggregate_row, axis=1)\n",
    "    return agg_train, agg_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_train, agg_test = aggregates(train_data, test_data)\n",
    "with open('cache/non_zero_stats.pkl', 'wb') as f:\n",
    "    pickle.dump([agg_train, agg_test], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
