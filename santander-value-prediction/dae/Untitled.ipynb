{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.8.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[22. 28.]\n",
      " [49. 64.]]\n"
     ]
    }
   ],
   "source": [
    "# Creates a graph.\n",
    "a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n",
    "b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n",
    "c = tf.matmul(a, b)\n",
    "# Creates a session with log_device_placement set to True.\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "# Runs the op.\n",
    "print(sess.run(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "import super_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "import seaborn as sns\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing\n",
    "from scipy.special import erfinv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "PATH=\"../input\"\n",
    "\n",
    "def rank_gauss(x):\n",
    "    \"\"\"\n",
    "    First step is to assign a linspace to the sorted features from 0..1, then apply \n",
    "    the inverse of error function ErfInv to shape them like gaussians, then I \n",
    "    substract the mean. Binary features are not touched with this trafo (eg. 1-hot ones). \n",
    "    This works usually much better than standard mean/std scaler or min/max.    \n",
    "    \"\"\"\n",
    "    \n",
    "    # x is numpy vector\n",
    "    N = x.shape[0]\n",
    "    temp = x.argsort()\n",
    "    rank_x = temp.argsort() / N\n",
    "    efi_x = erfinv(rank_x)\n",
    "    efi_x -= efi_x.mean()\n",
    "    return efi_x\n",
    "\n",
    "\n",
    "def load():\n",
    "    print('reading csvs')\n",
    "    df_train = pd.read_csv(os.path.join(PATH, \"train.csv\"))\n",
    "    df_test = pd.read_csv(os.path.join(PATH, \"test.csv\"))\n",
    "    dtypes = df_train.dtypes.to_dict()\n",
    "    print(f\"train shape: {df_train.shape}, test shape: {df_test.shape}\")\n",
    "\n",
    "    train_target = df_train['target'].values\n",
    "\n",
    "    ntrain = df_train.shape[0]\n",
    "    ntest  = df_test.shape[0]\n",
    "\n",
    "    ignored_columns = ['ID', 'target']\n",
    "    feature_columns = [c for c in df_train.columns if c not in ignored_columns]    \n",
    "    categorical_columns = []    \n",
    "    \n",
    "    # concatencate everything train + test...\n",
    "    df_all = pd.concat([df_train[feature_columns], df_test[feature_columns]]).astype(np.float32)\n",
    "    \n",
    "    dtype_counter = Counter()\n",
    "    unique_counter = Counter()\n",
    "    for c in feature_columns:\n",
    "        dtype_str = str(dtypes[c])\n",
    "        dtype_counter[dtype_str] += 1\n",
    "        unique = len(set(df_train[c].values))\n",
    "        unique_counter[unique] += 1\n",
    "        if unique == 1:\n",
    "            ignored_columns.append(c)\n",
    "            \n",
    "    # \n",
    "    # use ohe for categoricals and \n",
    "    # rank_gauss for the rest.\n",
    "    #\n",
    "    # df_all = pd.get_dummies(df_all, prefix='ohe_', columns=categorical_columns, sparse=True)\n",
    "\n",
    "    print(f\"categoricals: {len(categorical_columns)}\")\n",
    "    print(f\"train+test shape after ohe'ing {df_all.shape}\")\n",
    "            \n",
    "    not_categorical = [x for x in feature_columns \n",
    "                       if x not in (categorical_columns) and (x not in ignored_columns)]\n",
    "    \n",
    "    print(f\"ignored columns: {len(ignored_columns)}\")\n",
    "    print(f\"not categorical: {len(not_categorical)}\")\n",
    "    \n",
    "    p = super_pool.SuperPool()\n",
    "    out = p.map(rank_gauss, [df_all[c] for c in feature_columns])\n",
    "    df_all = pd.concat(out, axis=1)\n",
    "    print(df_all.shape)\n",
    "        \n",
    "    p.exit()\n",
    "    \n",
    "    #for c in not_categorical:\n",
    "    #for c in tqdm(feature_columns): # TODO\n",
    "    #    df_all[c] = rank_gauss(df_all[c].values)\n",
    "                    \n",
    "    # print('label encoding...')\n",
    "    # for c in categorical_columns:\n",
    "    #    df_all[c] = LabelEncoder.fit_transform(df_all[c])\n",
    "        \n",
    "    df_all = df_all[[c for c in df_all.columns if c not in ignored_columns]]\n",
    "    train_data = df_all.iloc[0:ntrain, :]\n",
    "    test_data  = df_all.iloc[ntrain:, :]\n",
    "\n",
    "    assert train_data.shape[0] == ntrain\n",
    "    assert test_data.shape[0]  == ntest\n",
    "\n",
    "    return train_target, categorical_columns, train_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading csvs\n",
      "train shape: (4459, 4993), test shape: (49342, 4992)\n",
      "categoricals: 0\n",
      "train+test shape after ohe'ing (53801, 4991)\n",
      "ignored columns: 258\n",
      "not categorical: 4735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8 CPUs: 100%|██████████| 4991/4991 [00:27<00:00, 182.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53801, 4991)\n"
     ]
    }
   ],
   "source": [
    "train_target, categorical_columns, df_train, df_test = load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4459,), 0, (4459, 4735), (49342, 4735), 4735, 4735)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.shape, len(categorical_columns), df_train.shape, df_test.shape, len(df_train.columns), len(df_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.24367585, 0.24434055, 0.24004841, ..., 0.24146532, 0.23124167,\n",
       "       0.22580724])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.values[0,:] #, np.sum(df_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"xx.pkl\", 'wb') as f:\n",
    "    pickle.dump([train_target, df_train, df_test, categorical_columns], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('xx.pkl', 'rb') as f:\n",
    "    train_target, df_train, df_test, categorical_columns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-4.983484e-11 -0.0126953125\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(df_all.values), np.sum(df_all.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_test]).astype(np.float32)\n",
    "train_idx, val_idx = train_test_split(range(df_all.shape[0]), test_size=0.2, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43040, 10761, (53801, 4735))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_idx), len(val_idx), df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import random\n",
    "\n",
    "class MyGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, categorical_columns, df, df_y=None, batch_size=128, shuffle=True):\n",
    "        self.categorical_columns = categorical_columns\n",
    "        self.X = df\n",
    "        self.y = df_y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle =shuffle\n",
    "        self.idxs = None\n",
    "        self.on_epoch_end()        \n",
    "        \n",
    "    def __len__(self):\n",
    "        # number of batches per epoch\n",
    "        return int(np.floor(self.X.shape[0] / self.batch_size))\n",
    "    \n",
    "    def _swap_noise(self, x):\n",
    "        shift = np.random.choice(x.shape[0], size=1)\n",
    "        out = np.copy(x)\n",
    "        x2 = np.roll(x, shift, axis=0)\n",
    "        # cols to randomizr        \n",
    "        for r in range(x.shape[0]):\n",
    "            cols = np.random.choice(x.shape[1], int(0.15 * x.shape[1]), replace=False)\n",
    "            for i in range(3):\n",
    "                out[r, cols] = x2[r, cols]\n",
    "                if np.random.sample() < 0.5:\n",
    "                    x2 = np.roll(x, shift, axis=0)\n",
    "                    out[r, cols] = x2[r, cols]\n",
    "        return out\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # generate one batch\n",
    "        batch_x = self.X.iloc[idx * self.batch_size:(idx + 1) * self.batch_size]                \n",
    "        #return batch_x, batch_x\n",
    "        return self._swap_noise(batch_x), batch_x\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.idxs = np.arange(self.X.shape[0])\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.idxs)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_g = MyGenerator(categorical_columns, df_all.iloc[train_idx])\n",
    "val_g = MyGenerator(categorical_columns, df_all.iloc[val_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo -- ohe categoricals\n",
    "# and modify original_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae1():\n",
    "    batch_size = 128\n",
    "    original_dim = 4735\n",
    "    latent_dim = 32\n",
    "    intermediate_dim = 128\n",
    "    epochs = 50\n",
    "    epsilon_std = 1.0\n",
    "\n",
    "    x = Input(batch_shape=(batch_size, original_dim))\n",
    "    h = Dense(intermediate_dim, activation='relu')(x)\n",
    "    z_mean = Dense(latent_dim)(h)\n",
    "    z_log_var = Dense(latent_dim)(h)\n",
    "\n",
    "    def sampling(args):\n",
    "        z_mean, z_log_var = args\n",
    "        epsilon = K.random_normal(shape=(batch_size, latent_dim), mean=0., stddev=epsilon_std)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    # note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "    z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])\n",
    "\n",
    "    # we instantiate these layers separately so as to reuse them later\n",
    "    decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "    decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "    h_decoded = decoder_h(z)\n",
    "    x_decoded_mean = decoder_mean(h_decoded)\n",
    "\n",
    "\n",
    "    def vae_loss(x, x_decoded_mean):\n",
    "        xent_loss = original_dim * metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return xent_loss + kl_loss\n",
    "\n",
    "\n",
    "    vae = Model(x, x_decoded_mean)\n",
    "    vae.compile(optimizer='rmsprop', loss=vae_loss)\n",
    "    return vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae1(original_dim):\n",
    "    #original_dim = 4735\n",
    "    latent_dim = 512\n",
    "    #intermediate_dim = 128        \n",
    "    \n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    encoded = Dense(latent_dim, activation='relu')(inputs)\n",
    "    #encoded = Dense(512, activation='relu')(encoded)\n",
    "    #decoded = Dense(512, activation='relu')(encoded)\n",
    "    x = encoded\n",
    "    decoded = Dense(original_dim, activation='linear')(x)\n",
    "    ae = Model(inputs, decoded)\n",
    "    ae.compile(optimizer='Adam', loss='mse')\n",
    "    return ae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53801, 4735)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "336/336 [==============================] - 57s 170ms/step - loss: 0.0416 - val_loss: 0.0335\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 58s 174ms/step - loss: 0.0329 - val_loss: 0.0312\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 53s 157ms/step - loss: 0.0310 - val_loss: 0.0303\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 56s 168ms/step - loss: 0.0305 - val_loss: 0.0309\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 54s 160ms/step - loss: 0.0300 - val_loss: 0.0302\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 53s 156ms/step - loss: 0.0300 - val_loss: 0.0310\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 53s 158ms/step - loss: 0.0297 - val_loss: 0.0302\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 53s 159ms/step - loss: 0.0305 - val_loss: 0.0299\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 53s 158ms/step - loss: 0.0294 - val_loss: 0.0302\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 53s 156ms/step - loss: 0.0292 - val_loss: 0.0302\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2b8d782860>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dae = dae1(df_all.shape[1])\n",
    "dae.fit_generator(generator=train_g,\n",
    "                  validation_data=val_g,\n",
    "                  epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_9 (None, 4735)\n",
      "dense_37 (None, 4735)\n",
      "dense_38 (None, 512)\n"
     ]
    }
   ],
   "source": [
    "for l in dae.layers:\n",
    "    print(l.name, l.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae2(original_dim, l1):\n",
    "    #original_dim = 4735\n",
    "    latent_dim = 512\n",
    "    #intermediate_dim = 128        \n",
    "    \n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    \n",
    "    encoded = l1(inputs)\n",
    "    encoded = Dense(512, activation='relu')(encoded)\n",
    "    \n",
    "    x = encoded\n",
    "    decoded = Dense(original_dim, activation='linear')(x)\n",
    "    ae = Model(inputs, decoded)\n",
    "    ae.compile(optimizer='Adam', loss='mse')\n",
    "    return ae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = dae.get_layer('dense_37')\n",
    "l1.trainable = False\n",
    "\n",
    "d2 = dae2(df_all.shape[1], l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "336/336 [==============================] - 53s 159ms/step - loss: 0.0335 - val_loss: 0.0298\n",
      "Epoch 2/10\n",
      "336/336 [==============================] - 54s 159ms/step - loss: 0.0293 - val_loss: 0.0294\n",
      "Epoch 3/10\n",
      "336/336 [==============================] - 55s 165ms/step - loss: 0.0287 - val_loss: 0.0289\n",
      "Epoch 4/10\n",
      "336/336 [==============================] - 53s 158ms/step - loss: 0.0283 - val_loss: 0.0288\n",
      "Epoch 5/10\n",
      "336/336 [==============================] - 53s 159ms/step - loss: 0.0281 - val_loss: 0.0287\n",
      "Epoch 6/10\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0279 - val_loss: 0.0286\n",
      "Epoch 7/10\n",
      "336/336 [==============================] - 53s 157ms/step - loss: 0.0276 - val_loss: 0.0284\n",
      "Epoch 8/10\n",
      "336/336 [==============================] - 53s 156ms/step - loss: 0.0274 - val_loss: 0.0284\n",
      "Epoch 9/10\n",
      "336/336 [==============================] - 57s 169ms/step - loss: 0.0272 - val_loss: 0.0283\n",
      "Epoch 10/10\n",
      "336/336 [==============================] - 54s 162ms/step - loss: 0.0272 - val_loss: 0.0281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f29768da0>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "d2.fit_generator(generator=train_g,\n",
    "                  validation_data=val_g,\n",
    "                  epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_11 (None, 4735)\n",
      "dense_37 (None, 4735)\n",
      "dense_39 (None, 512)\n",
      "dense_40 (None, 512)\n"
     ]
    }
   ],
   "source": [
    "for l in d2.layers:\n",
    "    print(l.name, l.input_shape)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dense_37', (None, 4735))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1.name, l1.input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = d2.get_layer('dense_39')\n",
    "l2.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae3(original_dim, l1, l2):\n",
    "    #original_dim = 4735\n",
    "    latent_dim = 512\n",
    "    #intermediate_dim = 128        \n",
    "    \n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    \n",
    "    encoded = l1(inputs)\n",
    "    encoded = l2(encoded)\n",
    "    encoded = Dense(512, activation='relu')(encoded)\n",
    "    \n",
    "    x = encoded\n",
    "    decoded = Dense(original_dim, activation='linear')(x)\n",
    "    ae = Model(inputs, decoded)\n",
    "    ae.compile(optimizer='Adam', loss='mse')\n",
    "    return ae\n",
    "\n",
    "\n",
    "d3 = dae3(df_all.shape[1], l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "336/336 [==============================] - 55s 162ms/step - loss: 0.0327 - val_loss: 0.0290\n",
      "Epoch 2/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0285 - val_loss: 0.0286\n",
      "Epoch 3/20\n",
      "336/336 [==============================] - 53s 157ms/step - loss: 0.0280 - val_loss: 0.0285\n",
      "Epoch 4/20\n",
      "336/336 [==============================] - 54s 160ms/step - loss: 0.0277 - val_loss: 0.0283\n",
      "Epoch 5/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0275 - val_loss: 0.0285\n",
      "Epoch 6/20\n",
      "336/336 [==============================] - 57s 170ms/step - loss: 0.0273 - val_loss: 0.0283\n",
      "Epoch 7/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0272 - val_loss: 0.0282\n",
      "Epoch 8/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0271 - val_loss: 0.0282\n",
      "Epoch 9/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0269 - val_loss: 0.0282\n",
      "Epoch 10/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0268 - val_loss: 0.0282\n",
      "Epoch 11/20\n",
      "336/336 [==============================] - 53s 157ms/step - loss: 0.0268 - val_loss: 0.0281\n",
      "Epoch 12/20\n",
      "336/336 [==============================] - 55s 164ms/step - loss: 0.0267 - val_loss: 0.0282\n",
      "Epoch 13/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0267 - val_loss: 0.0281\n",
      "Epoch 14/20\n",
      "336/336 [==============================] - 52s 155ms/step - loss: 0.0266 - val_loss: 0.0283\n",
      "Epoch 15/20\n",
      "336/336 [==============================] - 52s 155ms/step - loss: 0.0265 - val_loss: 0.0281\n",
      "Epoch 16/20\n",
      "336/336 [==============================] - 52s 154ms/step - loss: 0.0265 - val_loss: 0.0282\n",
      "Epoch 17/20\n",
      "336/336 [==============================] - 52s 156ms/step - loss: 0.0265 - val_loss: 0.0281\n",
      "Epoch 18/20\n",
      "336/336 [==============================] - 55s 164ms/step - loss: 0.0265 - val_loss: 0.0284\n",
      "Epoch 19/20\n",
      "336/336 [==============================] - 52s 155ms/step - loss: 0.0264 - val_loss: 0.0281\n",
      "Epoch 20/20\n",
      "336/336 [==============================] - 52s 155ms/step - loss: 0.0264 - val_loss: 0.0281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f293c1d68>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d3.fit_generator(generator=train_g,\n",
    "                  validation_data=val_g,\n",
    "                  epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_12\n",
      "dense_37\n",
      "dense_39\n",
      "dense_41\n",
      "dense_42\n"
     ]
    }
   ],
   "source": [
    "for l in d3.layers:\n",
    "    print(l.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "d3.save('d3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "d3 = load_model('d3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras.engine.topology.InputLayer object at 0x7f66ab368710> (None, 4735)\n",
      "<keras.layers.core.Dense object at 0x7f66ab3688d0> (None, 4735)\n",
      "<keras.layers.core.Dense object at 0x7f66ab368a20> (None, 512)\n",
      "<keras.layers.core.Dense object at 0x7f66ab368898> (None, 512)\n",
      "<keras.layers.core.Dense object at 0x7f66ab368b38> (None, 512)\n"
     ]
    }
   ],
   "source": [
    "for l in d3.layers:\n",
    "    print(l, l.input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3 = d3.get_layer('dense_41')\n",
    "l3.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dae4(original_dim, l1, l2, l3):\n",
    "    #original_dim = 4735    \n",
    "    #intermediate_dim = 128        \n",
    "    \n",
    "    inputs = Input(shape=(original_dim,))\n",
    "    \n",
    "    encoded = l1(inputs)\n",
    "    encoded = l2(encoded)\n",
    "    encoded = l3(encoded)\n",
    "    \n",
    "    x = encoded\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(512, activation='relu')(x)\n",
    "    x = Dense(1, activation='linear')(x)\n",
    "    ae = Model(inputs, x)\n",
    "    ae.compile(optimizer='Adam', loss='mse')\n",
    "    return ae\n",
    "\n",
    "\n",
    "model = dae4(df_all.shape[1], l1, l2, l3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2 = df_all.iloc[:df_train.shape[0]]\n",
    "df_target2 = np.log1p(train_target)\n",
    "df_target2 /= np.mean(df_target2)\n",
    "\n",
    "#train/test split.\n",
    "train_idx2, val_idx2 = train_test_split(range(df_train2.shape[0]), test_size=0.05, random_state=0, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4236, 4735), (4236,), (223, 4735), (223,))"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train2.iloc[train_idx2].shape, df_target2[train_idx2].shape, df_train2.iloc[val_idx2].shape, df_target2[val_idx2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4236 samples, validate on 223 samples\n",
      "Epoch 1/20\n",
      "4236/4236 [==============================] - 1s 147us/step - loss: 2.2694 - val_loss: 0.0263\n",
      "Epoch 2/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0204 - val_loss: 0.0144\n",
      "Epoch 3/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0145 - val_loss: 0.0145\n",
      "Epoch 4/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0130 - val_loss: 0.0140\n",
      "Epoch 5/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0124 - val_loss: 0.0128\n",
      "Epoch 6/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0118 - val_loss: 0.0128\n",
      "Epoch 7/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0114 - val_loss: 0.0126\n",
      "Epoch 8/20\n",
      "4236/4236 [==============================] - 1s 135us/step - loss: 0.0110 - val_loss: 0.0129\n",
      "Epoch 9/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0107 - val_loss: 0.0124\n",
      "Epoch 10/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0107 - val_loss: 0.0128\n",
      "Epoch 11/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0106 - val_loss: 0.0137\n",
      "Epoch 12/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0103 - val_loss: 0.0122\n",
      "Epoch 13/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0100 - val_loss: 0.0123\n",
      "Epoch 14/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0102 - val_loss: 0.0122\n",
      "Epoch 15/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0099 - val_loss: 0.0123\n",
      "Epoch 16/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0097 - val_loss: 0.0120\n",
      "Epoch 17/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0096 - val_loss: 0.0128\n",
      "Epoch 18/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0097 - val_loss: 0.0131\n",
      "Epoch 19/20\n",
      "4236/4236 [==============================] - 1s 133us/step - loss: 0.0094 - val_loss: 0.0125\n",
      "Epoch 20/20\n",
      "4236/4236 [==============================] - 1s 134us/step - loss: 0.0094 - val_loss: 0.0125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2f1d0344a8>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(df_train2.iloc[train_idx2], df_target2[train_idx2],\n",
    "         validation_data=(df_train2.iloc[val_idx2], df_target2[val_idx2]),\n",
    "         epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    " #df_train2.iloc[val_idx2].shape, df_target2[val_idx2].shape\n",
    "pred_val = model.predict(df_train2.iloc[val_idx2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.78563305762031"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mean_squared_error(df_target2[val_idx2]*14.9, pred_val*14.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53801/53801 [==============================] - 6s 114us/step\n"
     ]
    }
   ],
   "source": [
    "m2 = Model(input=d3.input, outputs=d3.layers[-2].output)\n",
    "p = m2.predict(df_all, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('out.pkl', 'wb') as f:\n",
    "    pickle.dump(p, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "  491520/11490434 [>.............................] - ETA: 1:51"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-a7203e2578a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the VAE on MNIST digits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float32'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/keras/datasets/mnist.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     21\u001b[0m     path = get_file(path,\n\u001b[1;32m     22\u001b[0m                     \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://s3.amazonaws.com/img-datasets/mnist.npz'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                     file_hash='8a61469f7ea1b51cbae51d4f78837e45')\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/site-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget_file\u001b[0;34m(fname, origin, untar, md5_hash, file_hash, cache_subdir, hash_algorithm, extract, archive_format, cache_dir)\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_progress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mURLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1007\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    869\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/miniconda2/envs/avito/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \"\"\"\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 631\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if False:\n",
    "    # train the VAE on MNIST digits\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train = x_train.astype('float32') / 255.\n",
    "    x_test = x_test.astype('float32') / 255.\n",
    "    x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "    x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "\n",
    "    vae.fit(x_train, x_train,\n",
    "            shuffle=True,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # build a model to project inputs on the latent space\n",
    "    encoder = Model(x, z_mean)\n",
    "\n",
    "    # display a 2D plot of the digit classes in the latent space\n",
    "    x_test_encoded = encoder.predict(x_test, batch_size=batch_size)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(x_test_encoded[:, 0], x_test_encoded[:, 1], c=y_test)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # build a digit generator that can sample from the learned distribution\n",
    "    decoder_input = Input(shape=(latent_dim,))\n",
    "    _h_decoded = decoder_h(decoder_input)\n",
    "    _x_decoded_mean = decoder_mean(_h_decoded)\n",
    "    generator = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "    # display a 2D manifold of the digits\n",
    "    n = 15  # figure with 15x15 digits\n",
    "    digit_size = 28\n",
    "    figure = np.zeros((digit_size * n, digit_size * n))\n",
    "    # linearly spaced coordinates on the unit square were transformed through the inverse CDF (ppf) of the Gaussian\n",
    "    # to produce values of the latent variables z, since the prior of the latent space is Gaussian\n",
    "    grid_x = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "    grid_y = norm.ppf(np.linspace(0.05, 0.95, n))\n",
    "\n",
    "    for i, yi in enumerate(grid_x):\n",
    "        for j, xi in enumerate(grid_y):\n",
    "            z_sample = np.array([[xi, yi]])\n",
    "            x_decoded = generator.predict(z_sample)\n",
    "            digit = x_decoded[0].reshape(digit_size, digit_size)\n",
    "            figure[i * digit_size: (i + 1) * digit_size,\n",
    "                   j * digit_size: (j + 1) * digit_size] = digit\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(figure, cmap='Greys_r')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
